# Progress Log

## 2025‑04‑19
- start github recording 
- prepare learning material
- using mmWave Demo Visualizer to connect Radar
- How to use mmWave Demo Visualizer
- mmWave Demo Visualizer
This is the TI Gallery APP for configuring mmWave sensors and visualizing point cloud objects generated by the mmWave SDK demo. This app is meant to be used in conjunction with the mmWave SDK demo running on the TI Evaluation module (EVM) for mmWave devices. This app can also be reached via TI Gallery – search for mmWave_Demo_Visualizer. The TI Gallery app is a browser-based app and can be run on any PC operating system (Windows®, Linux®, or macOS®), and TI recommends running the app using the Chrome® browser for the best plotting performance.

Note about compatibility
This version of app is compatible with mmWave SDK versions 3.5, 3.4, 3.3, 3.2.x and 3.1.x. Users should select the right SDK version in this app as the mmWave demo version running on the mmWave device. If user chooses to upgrade the custom configuration for a given mmWave device based off previous mmWave SDK release so that they are able to run the same config for the same platform with latest mmWave SDK release, then then there is a script mmwDemo_<platform>_update_config.pl provided in the mmwave_sdk_<ver>/packages/ti/demo/<platform>/mmw/profiles directory that they can use to convert the configuration file from older release to a compatible version for the new release. See mmWave SDK User Guide for more details.

Basic steps for running the app are listed here. For more detailed information, please refer to the mmWave Demo Visualizer User's guide.

Steps for using this App

- If this is the first time you are using this App, you may be requested to install a plug-in and the TI Cloud Agent Application.
- Power up the mmWave Sensors and load/run the mmW Demo located at mmwave_sdk/_<ver>/packages/ti/demo/<platform>/mmw. Refer to the mmWave SDK user guide in the mmWave SDK package for more information.
- Once the demo is running on the mmWave sensors and the USB is connected from the board to the PC, the app will try to automatically detect the COM ports for your device. If auto-detection doesn't work, then you will need to configure the serial ports in this App. In the App, go to the Menu->Options->Serial Port.
- CFG_port: Use COM port number for "XDS110 Class Application/User UART": Baud: 115200
- Data_port: Use COM port "XDS110 Class Auxiliary Data port": Baud: 921600 Hint: Navigate to the device manager on the windows PC to locate the COm port numbers
- At this point this app will automatically try to connect to the target (mmWave Sensor). If it does not connect or if the connection fails, you should try to connect to the target by clicking in the bottom left corner of this App.
- After the App is connected to the target, you can select the configuration parameters in this App (Frequency Band, Platform, etc) in the "Scene Selection" area of the CONFIGURE tab.
- Besides selecting the configuration parameters, you should select which plots you want to see. This can be done using the "check boxes" in the "Plot Selection" area.
- Once the configuration is selected, you can send the configuration to the device (use "SEND CONFIG TO MMWAVE DEVICE" button) or save the configuration to the PC (use "SAVE CONFIG TO PC" button).
- After the configuration is sent to the device, you can switch to the PLOT view/tab and the plots that you selected will be shown.
- On the plots tab, user has access to real time tuning tab and advanced commands tab to tune the processing chain. The commands are sent immediately to the device (without sensorStop) and effect of those commands (if any) should be visible in the displayed plots
- You can switch back from "Plot" tab to "Configure" tab, reconfigure your "Scene Selection" and/or "Plot Selection" and re-send the configuration to the device to try a different profile. After a new configuration has been selected, just press the "SEND CONFIG TO MMWAVE DEVICE" button again and the device will be reconfigured. This can be done without rebooting the device.If you change the parameters in the "Setup Details", then you will need to take further action before trying the new configurations
If Platform is changed: make sure the COM ports match the TI EVM/platform you are trying to configure and visualizer
If SDK version is changed: make sure the mmW demo running on the connected TI EVM matches the selected SDK version in the GUI
If Antenna Config is changed: make sure the TI EVM is rebooted before sending the new configuration.
If board is rebooted, follow the steps starting from 1 above.

Advanced options
- User can configure the device from their own configuration file or the saved app-generated configuration file by using the "LOAD CONFIG FROM PC AND SEND" button on the PLOTS tab. Make sure the first two commands in this config file are "sensorStop" followed by "flushCfg".
- User can temporarily pause the mmWave sensor by using the "STOP" button on the plots tab. The sensor can be restarted by using the "START" button. In this case, sensor starts again with the already loaded configuration and no new configuration is sent from the App.
- It is recommended to always use the online/cloud version of the Visualizer for the mmWave experience with the TI devices but we do understand that users may not always have access to internet connection while trying to evaluate the mmWave devices, especially in the field trial. For such scenarios, there is a link for offline version available under “Help->Download or Clone Visualizer”.
- If the user desires to save the incoming processed stream from the mmWave device for some offline analysis while its getting plotted, users can use the “Record Start” button in the plots tab. More details can be found in the mmWave Demo Visualizer User's guide. Note: This feature requires the browser version requirement to be as mentioned here: ti-widget-streamsaver

  ## 2025‑04‑20
- Environmental preparation
Install and verify Python 3.13.2 on Windows, and confirm the environment through python --version and python -m pip --version; Upgrade pip to the latest version to ensure stable installation of dependencies.
- Dependent deployment
install the core library using python -m pip install numpy pyserial pyqtgraph; Import and verify the versions of each library in the interactive environment to ensure there are no errors.
- Installation and Startup of TI MMWAVE SDK 3
Download and install mmWave-SDK 3 (03.06.02.00-LTS), and configure the 32-bit compatibility library.
Test the CLI interface in the command line through mmwave_demo_cli.exe;
Download and extract mmWave Demo Visualizer separately, run visualizer_server.bat, and verify the radar data in the browser through the graphical interface.
- Analysis of the AWR1843 configuration file
Conduct an in-depth analysis of the meanings of parameters such as profileCfg, chirpCfg, frameCfg, CFAR, AoA/Range/Doppler FoV in.cfg;
Verify the distance/speed resolution and the theoretical values of the maximum unambiguous range and speed to ensure that the configuration meets the application requirements.

## 2025‑05-01
- Due to the impact of the holiday and the exam week, the project's progress has been put on hold for several days. Today's task is to build the platform and the toolchain. Install and open CCS, confirm that the corresponding mmWave SDK and Radar Toolbox have been installed, connect the EVM development board, and learn to use Debug Probe to download the firmware and view the serial port log.
- today read some documents on the mmWave SDK. And try the example on the development tools. ![image](https://github.com/user-attachments/assets/9ab40783-45bd-497e-b841-840555809775)
## 2025‑05-02
- By using the mmWave demo visualizer, I got 2 .dat documents. One is a normal record, and the other is recorded when static clutter is eliminated. Which record my walking back and forth in front of the radar. However,  both VS code and Pycharm can not open them. I need to ask my tutor for help.![image](https://github.com/user-attachments/assets/24109ab2-565d-43ef-9060-e22968d206cc)
- It's true that the web of mmWave demo visualizer can do a large mount of work. I still need some other ways to deal with these data, to train the model. I am afraid I need to learn some knowledge about machine learning.
- The demo visualizer not only produced the .dat document, but also .cfg![image](https://github.com/user-attachments/assets/df8279e7-17ac-46d9-9297-fe63fbb75ef4)

  
## 2025‑05-07
- <img width="1280" alt="微信图片_20250507154627" src="https://github.com/user-attachments/assets/92bca619-75cc-42ed-9654-72712a7b6ec0" />
I did 21 times update of my coding on python, However it can't do nothing. I use the cfg which got from mmWave demo visualizer. And do some changes of the details. Just don't work. Just no outputs. I was really tired to change it again and again. Later I may be use Matlab to deal with these fking shit. Why everytime I using the fk python, it let me download something. Everytime!!!!! I don't even know what are their functions.
![image](https://github.com/user-attachments/assets/a09db161-4921-458a-8292-1bea288a1e58)
By using demo visulizer, When there are no moving objects, the relative power will remain at around 40dB. Once a moving object appears (take the swing of an arm as an example), the relative power will immediately rise to 60-80dB depending on the speed and the size of the distance. This is achieved through the algorithm of eliminating static clutter.

## 2025‑05-08
- On that day, the focus was on integrating the data acquisition and visualization pipeline of the AWR1843AOP millimeter-wave radar. Firstly, based on the official TLV protocol of TI, the compatibility reconfiguration of the CLI instruction script and the serial port receiving script was completed to ensure the parallel and stable operation of the two ports. Subsequently, in view of the API differences between PyQtGraph and Open3D, the import errors of QApplication were quickly located and fixed, and the interaction of the 3D point cloud interface was restored. Next, I designed and verified the universal frame header and TLV block parsing module, enabling the same set of scripts to be seamlessly adapted to both IWR6843 and AWR1843AOP devices. Finally, through on-site collection of multi-scenario data in the laboratory, end-to-end testing was completed to confirm that the point cloud rendering and frame rate met the real-time requirements. At the same time, the feasibility of native Python parsing of the original.dat file was evaluated, laying the foundation for the subsequent algorithm development.

https://github.com/user-attachments/assets/586cbce0-f92e-40dd-9765-55d19d0a6f12

## 2025‑05-09
- Today, the focus shifts to enhancing data processing efficiency and conducting algorithm pre-research. Firstly, the verified TLV parsing module is encapsulated as a Python package to complete the buffering management and concurrent read and write optimization of continuous flow data, and the parsing throughput rate is tested under different data scales. Subsequently, based on the processed point cloud data, the CFAR object detection and background noise filtering processes were initially implemented, and two filtering strategies, namely the sliding window and the statistical model, were compared. Immediately afterwards, a small-scale real-time demonstration environment was set up. The radar startup, configuration and acquisition were automatically controlled through Python scripts, and the stability of the scripts was verified. Finally, the performance test results of today were sorted out, the selection of classification algorithms and GPU acceleration schemes for the next stage were formulated, and the technical routes of multi-target tracking and micro-Doppler feature extraction were clarified.



https://github.com/user-attachments/assets/3e56d54b-3c71-453a-b472-a76d9cf33eaf

## 2025‑05-12
- At the beginning, I opened the original csv_to_3d3.py script, combined with the CSV samples at hand, and compared the effect displayed by each frame of data. I found that all point clouds appeared concentratedly at the 0.000th second, and it was impossible to distinguish the time evolution at all. After browsing the code and confirming, it directly uses subFrame as the grouping key, while in our single-frame mode, this field is always zero, resulting in the aggregation of all frame data together.
- When deeply debugging the CSV file, I noticed that there was a detIdx field in each row of records, which was reset to 0 during the first detection of each frame. So I decided to use it to divide frames: when encountering detIdx == 0, a new frame was opened and the subsequent coordinate points were classified into this new frame. In this way, even if the subframes remain unchanged, the point cloud sequences of each frame can still be accurately separated.
- Next, I extract the timeStamp of the first record of each frame in the script and calculate the true time difference between two adjacent frames. Since the timestamps of some frames are the same (subframes share the same moment), I forcibly set it to 1 millisecond when the difference is less than or equal to zero to ensure that the time axis increases strictly. If the parsing fails, it will return to the default 50 ms. After accumulating all the differences, a rel_times list of the same length as the number of frames was obtained, which was used as the driving sequence of the animation.
- After modifying the data framing and timeline parts, I reconstructed the timer callback: Each time the QElapsedTimer is triggered, the current elapsed seconds are compared with rel_times[idx+1], and the index is advanced by one frame only when it exceeds. This logic enables the scatter plot to be presented from the first frame and gradually switch to the next frame as real time progresses, completely eliminating the problem of all frames being stacked at once.
- Multiple tests were conducted on real data. The playback effects at common frame rates of 50 ms and 71.429 ms were verified successively, and the scenarios where multiple subframes shared timestamps were also tested. The results show that the new script can smoothly and accurately display the radar point cloud in the chronological order recorded in the CSV. Whether in single-frame mode or multi-subframe configuration, it can correctly frame and broadcast.
- So far, the entire process from "debugging and positioning" → "frame reconfiguration" → "timeline calculation" → "timed playback" → "test verification" has been completed, and the script has achieved the expected goal. Next, I plan to incorporate playback control and performance optimization to further enhance the visual experience.



https://github.com/user-attachments/assets/4e01902e-ed21-4507-be0e-52726a65e34d

## 2025-07-07
- End the exams and the vacation, and then start to advance the project. During the testing of the code, an error was encountered. After checking, it was found to be a path error. After moving the file to the correct location, the code ran normally. Then, relevant radar movement data needs to be collected.

## 2025-07-08
- The CSV fields (x, y, z, v, etc. representing physical quantities) were confirmed and previewed, and the input and output of clustering and speed threshold classification were clarified. 
Wrote and debugged a Python script, using DBSCAN to cluster the three-dimensional point cloud, calculate the average speed, and determine "dangerous scooters" based on a speed of > 4 m/s.

## 2025-07-16
- First, use the final_radar_cfg_document.cfg configuration to start the mmWave Demo and generate a new .dat file. Then, specify the .dat file name in dat_to_csv2.py and write the frame interval (50 ms) to output a CSV with a timestamp. Then, load this CSV in 3D_clustering.py and perform clustering and segmentation on the point cloud based on DBSCAN. Finally, read the same CSV again in csv_to_3d3.py and draw a 3D point cloud with speed labels after clustering to achieve integrated detection and visualization.
-set the suitable cfg
<img width="531" height="252" alt="image" src="https://github.com/user-attachments/assets/d3fc8b4f-43b9-4aba-938c-58cb43ab1d72" />

## 2025-07-28
Debugging of DBSCAN clustering algorithm

- Different parameters of eps (0.3, 0.6) and min_samples (35) were tested, and it was determined that for pedestrians and electric scooters, eps≈0.5 and min_samples=3 to 4 were the most appropriate.

- The eps selection has been optimized for the slender structure (handlebars/base) of the scooter. It is recommended that eps=0.55 and min_samples=3.

Behavior recognition extension

- It was discussed how to distinguish normal driving from raising hands actions through the z_range and x_range of clusters.

- In the cluster analysis phase of the code, a structural judgment logic has been added to detect the base of the scooter or the gesture of raising hands.

Code update and optimization

- The structure analysis and classification process in the for cluster_id loop has been improved.

- The optional visualization function plot_clusters() is provided to assist in debugging eps and clustering effects.

## 2025-07-29
Set the suitable number for e-scooter(bottom & humen)

## 2025-07-30
- Point Cloud Clustering and Visualization Animation Optimization 

The three-dimensional point cloud clustering based on DBSCAN and the dynamic bounding box visualization have been realized. Each frame is automatically updated, facilitating the observation of the spatial distribution and movement process of pedestrians and electric scooters. 

To address the issues of overly rapid animation and sudden changes in the coordinate axis range, the animation speed has been successfully optimized, and a global fixed coordinate axis has been implemented, thereby enhancing the intuitiveness and professionalism of data presentation. 

- Code debugging and function expansion 

The common errors such as function definitions, file paths, and parameter configurations have been gradually corrected. 

The definition of the bounding box has been clarified: The bounding box of each clustering cluster is the smallest enclosing cube for all the points of that cluster (automatically generated based on the maximum and minimum values of x/y/z).


https://github.com/user-attachments/assets/bd1811ac-aaac-42c8-9866-a27c048ac53e

## 2025-08-08
- Debug

## 2025-08-13
- Finished the mearsurement of E-scooter(Neuron). Got the Data of length, width and height of electric scooters. And draw the Top and side views.
- ![c20be76b8aeb9d6fdaee5d077d42b0e](https://github.com/user-attachments/assets/46b78811-5ed9-4e85-b625-a7495b0e965e)
![d2bfedb17e99c0c6e0e6e3c3ae654c2](https://github.com/user-attachments/assets/ed0e86df-82fb-46d4-bb0b-7119a7c6f217)
![5de46a6e0bc4d27fab936efcd360b23](https://github.com/user-attachments/assets/69b920eb-d4e0-4be3-9cad-71762c699e53)

- 1) Goals
Integrate the L-shaped e-scooter model + human cylinder with the existing point-cloud animation.

Add Doppler/radial velocity computation/overlay for a roadside radar geometry.

Fix logic to detect-then-track so no target is drawn when the data has none
- 2) Key Changes
Updated 3D_clustering_with_boxes.py with clearly marked blocks:

=== NEW: ... === (initial integration)

=== NEW(v2): ... ===, # === CHANGED(v2): ... === (detect-then-track)
- 3) Interfaces & Defaults
animate(...): interval_ms=1000, fc=77e9, tracker alpha=0.6, beta=0.2, max_miss=0.

Detection gates (DETECT_CFG): width/length/height ranges as above; zmin_max=0.25 m; min_points=20; score_thresh=4.0.
- 4) Expected Behavior
With target: point cloud + AABB + rigid L+human overlay; Doppler goes through zero near closest approach.

No target: only point cloud + AABB; “No target detected” text; no rigid model/Doppler.
- 5) Quick Validation
Axes & units consistent; correct frame splitting with detIdx==0 at frame start; visible Doppler zero-crossing.
# The Model:
<img width="1196" height="1242" alt="scooter_human_model" src="https://github.com/user-attachments/assets/a6670ca0-b268-4b6d-91b6-8cd9da35145d" />
<img width="1429" height="880" alt="scooter_human_side_view" src="https://github.com/user-attachments/assets/42f5f517-90a8-4de7-9b48-e28bf6813257" />

## 2025-08-16
- Make the model (L-shaped e-scooter + human cylinder) work end-to-end on our point-cloud
- Add a pedestrian model, and unify detection
- Add overspeed rule = 4 m/s for e-scooters
- Data_collection_plan
  [Data_collection_plan.pdf](https://github.com/user-attachments/files/21810694/Data_collection_plan.pdf)
<img width="907" height="1326" alt="image" src="https://github.com/user-attachments/assets/122ec6b6-c0d1-48ed-9e5f-32321fef0162" />

## 2025-08-24
- .dat → .csv blank outputs & pipeline debugging

Symptoms: Some .dat files yield blank/incomplete .csv.

Actions:

Confirmed source: TI mmWave Demo Visualizer exports.

Reworked read/write paths and unpack logic per your project directory.

Identified root cause: parser wasn’t generalizing across different .cfg parameters; proposed a “read from .cfg” adaptive approach.


## 2025-09-02 
- Generalized .dat + .cfg → .csv converter

Need & changes:

Original script failed for some .dat/.cfg pairs.

Switch to parsing the paired .cfg (sample rate, antennas/virtual channels, chirp/frame, etc.) to drive decoding.

## 2025-09-03

Point-cloud coordinates & pedestrian speed correction (core issue)

Scene geometry (confirmed for calibration):

Radar mounted perpendicular to road centerline, with a 3 m lateral offset from the centerline.

Pedestrians walk along the centerline (far→near, then near→far).

Motion is planar; units meters.

Symptoms:

Pedestrian speeds are grossly over-estimated (often 10–20 m/s), with cumulative “10→30→50 m/s” across repeated experiments.

Confirm detIdx == 0 means a new frame → velocity chains must reset at frame boundaries, otherwise historical displacement leaks into current speed.

Constraint: Do not change axes; fix speed only.

Data: Among four CSVs, twopeople_parallel.csv is the most complete for detection; all four show speed bias.

## 2025-09-05

Clustering/correlation parameters

Raster clustering: cell=0.7 m, min_points=3;

Association threshold: assoc_gate = max(1.8, 5*dt_med*2).

Validate at keep_walking.csv

Results: Stable identification, velocity of 0.8–2 m/s; Missed detections are reduced.

Verify at old_man_pass.csv

Results: Slow passage can be identified; The extremely slow section (<0.3 m/s) was judged to be immobile/unstable.

Verify at twopeople_parallel.csv

Results: Two people can be tracked simultaneously in parallel, and the ID is stable. Speed 0.5–2.1 m/s.

Initial test in keep_run.csv

Results: The running segment was not confirmed (speed often > 3 m/s, exceeding the "walking limit" and not scoring).

Relaxation of parameters for running (Scheme A)

WALK_SPEED_HI 3.0→7.0 m/s;

SPEED_SANITY_MAX 5.0→9.0 m/s;

ASSOC_GATE_BASE_M 1.8→2.4 m.

Retest at keep_run.csv

Results: Running back and forth can be stably recognized and locked; The velocity distribution was 2.5–5 m/s, and there was no more 10–20 m/s anomaly.

Optional Logic Tests (Protocol B)

Remove the "upper speed limit" in the confirmation condition, and keep only v ≥ 0.3 m/s and the minimum duration and height threshold.

Result: Both walk/run can be confirmed, which is convenient for subsequent labeling according to speed (walk/jog/run).
